# Pipeline Batch Bovespa - Tech Challenge (Fase 2)

Este reposit√≥rio cont√©m o componente de **Ingest√£o** de um pipeline de dados completo para a B3. O objetivo √© extrair dados hist√≥ricos/di√°rios, convert√™-los para o formato otimizado Parquet e armazen√°-los no AWS S3 seguindo uma estrutura de particionamento por data.

## üìê Arquitetura do Pipeline

- Ingest√£o: `ingestor.py` usa `yfinance` para extrair pre√ßos e grava arquivos Parquet particionados por dia em S3 (padr√£o: `raw/dt=YYYY-MM-DD/`).
- Transforma√ß√£o: AWS Glue (Spark) processa o raw e escreve o refined, particionando por data/ticker para consulta via Athena.
- Consumo: Athena / Spark para an√°lises e agrega√ß√µes.

> Observa√ß√£o: a parti√ß√£o no raw usa a chave `dt=YYYY-MM-DD` (consistente com o padr√£o do reposit√≥rio).
## üéØ Decis√µes do Projeto

Para atender aos requisitos de Machine Learning Avan√ßado e an√°lise de dados, foram eleitos **10 ativos (Blue Chips)** que representam diferentes setores da economia brasileira, garantindo uma base de dados rica para as fases subsequentes de ETL e an√°lise:

1. **VALE3.SA** (Minera√ß√£o)
2. **PETR4.SA** (Petr√≥leo)
3. **ITUB4.SA** (Setor Banc√°rio)
4. **BBDC4.SA** (Setor Banc√°rio)
5. **ABEV3.SA** (Consumo/Bebidas)
6. **WEGE3.SA** (Ind√∫stria/Bens de Capital)
7. **BBAS3.SA** (Setor Banc√°rio)
8. **B3SA3.SA** (Servi√ßos Financeiros/Bolsa)
9. **RENT3.SA** (Servi√ßos/Loca√ß√£o de Ve√≠culos)
10. **SUZB3.SA** (Papel e Celulose)

## üèóÔ∏è Arquitetura de Ingest√£o

- **Linguagem:** Python 3.10+
- **Bibliotecas Base:** `yfinance` (extra√ß√£o), `pandas` + `pyarrow` (processamento e Parquet), `boto3` (AWS S3).
- **Gerenciador de Depend√™ncias:** `Poetry`.
- **Formato de Sa√≠da:** Parquet (Requisito 2).
- **Particionamento (ingest√£o raw):** `raw/dt=YYYY-MM-DD/` ‚Äî o `ingestor.py` gera um arquivo consolidado por data (`b3_stocks.parquet`). A separa√ß√£o por `ticker` e o particionamento adicional s√£o realizados no **Glue** (zona `refined`), que grava `refined/date=YYYY-MM-DD/ticker=.../` para otimizar consultas via Athena (Requisito 6).

## üöÄ Como Executar

### Pr√©-requisitos

- Poetry instalado (`pip install poetry`)
- Conta na AWS com permiss√µes de S3 e Glue.

### Configura√ß√£o (.env)

Crie um arquivo `.env` na raiz do projeto seguindo o modelo abaixo para que o script possa se autenticar na AWS e identificar o destino dos dados:

```env
S3_BUCKET=nome-do-seu-bucket
AWS_ACCESS_KEY_ID=sua_key
AWS_SECRET_ACCESS_KEY=seu_secret
AWS_DEFAULT_REGION=us-east-1
# AWS_SESSION_TOKEN=token_se_necessario (comum em contas Lab/Academy)
```

### Instala√ß√£o

```powershell
poetry install
```

### Execu√ß√£o Local (Valida√ß√£o)

Para testar a gera√ß√£o de arquivos Parquet particionados na sua m√°quina:

#### Uso (par√¢metros)

- `tickers` (opcional, positional): lista de s√≠mbolos a serem baixados. Se omitido, usa a lista padr√£o definida em `ingestor.py`.
- `--period <period>`: janela de tempo para o download (ex.: `1d`, `5d`, `30d`, `6mo`). Use `30d` para os √∫ltimos 30 dias.
- `--local`: flag opcional ‚Äî grava os arquivos localmente em `data/` ao inv√©s de fazer upload para S3.

#### Exemplo ‚Äî 30 dias (apenas este exemplo)

```powershell
# Gera arquivos locais para os √∫ltimos 30 dias (parti√ß√µes di√°rias)
poetry run python ingestor.py --period 30d --local
```

#### Exemplo ‚Äî data √∫nica (uso do `--date`)

```powershell
# Baixa e grava apenas os dados do dia 2026-01-16 (inclusivo)
poetry run python ingestor.py --date 2026-01-16 --local
```

Os arquivos ser√£o gerados em: `data/raw/dt=YYYY-MM-DD/b3_stocks.parquet` (arquivo consolidado por data). A separa√ß√£o por `ticker` e o refinamento s√£o realizados no Glue e resultar√£o em objetos como `data/refined/date=YYYY-MM-DD/ticker=VALE3.SA/...` ‚Äî ver se√ß√£o 'Transforma√ß√µes / Glue' para detalhes.

### Execu√ß√£o para S3 (Produ√ß√£o)

Com o `.env` configurado, basta rodar:

```powershell
# Ingest√£o di√°ria (1d) para todos os ativos eleitos
poetry run python ingestor.py --period 1d
```

> Observa√ß√£o: o particionamento por `ticker` e as agrega√ß√µes exigidas pelo challenge s√£o executadas no **Glue (zona refined)** ‚Äî o raw √© consolidado por data; o Glue escreve `refined/date=YYYY-MM-DD/ticker=.../` para otimizar consultas via Athena.

## ‚è±Ô∏è Intraday / `--interval` ‚Äî comportamento e compatibilidade (IMPORTANTE)

- Armazenamento padr√£o: **timestamps s√£o gravados em UTC** como `TIMESTAMP` (milissegundos) ‚Äî este √© o formato recomendado e compat√≠vel com AWS Glue / Spark. Os arquivos continuam **particionados por dia** em `dt=YYYY-MM-DD` (calendar day).
- O `yfinance` suporta intraday via `--interval` (ex.: `1m`, `5m`, `60m`); disponibilidade hist√≥rica varia por intervalo e ticker ‚Äî intervals intradi√°rios frequentemente t√™m hist√≥rico limitado.
- O `ingestor.py` mant√©m **a granularidade intradi√°ria** nas linhas (ex.: v√°rios registros no mesmo `dt` com horas diferentes) e **normaliza para UTC antes de gravar**.

### Exemplos de uso üìå

- Baixar barras hor√°rias (√∫ltimos 7 dias) e gravar localmente:

```powershell
poetry run python ingestor.py VALE3.SA --period 7d --interval 60m --local
```

- Exemplo di√°rio (comportamento legado permanece):

```powershell
poetry run python ingestor.py --period 30d --local
```

### Como o timestamp √© exposto aos consumidores (Spark/Glue) üîß

- Armazenamos um `trade_date` do tipo `TIMESTAMP` (valores representam instantes em UTC).
- Para consultas/visualiza√ß√£o em hor√°rio brasileiro (America/Sao_Paulo) use fun√ß√µes de convers√£o no momento da leitura ‚Äî n√£o modifique os valores armazenados.

Spark (leitura + convers√£o para BRT):

```sql
-- Athena / Spark SQL example
SELECT
  from_utc_timestamp(trade_date, 'America/Sao_Paulo') AS trade_date_brt,
  to_date(from_utc_timestamp(trade_date, 'America/Sao_Paulo')) AS trade_day,
  ticker, open, close, volume
FROM parquet_table
WHERE dt = '2026-01-09';
```

PySpark (convers√£o / extrair dia):

```python
df = spark.read.parquet('s3://.../raw/')
df = df.withColumn('trade_date_brt', F.from_utc_timestamp(F.col('trade_date'), 'America/Sao_Paulo'))
df = df.withColumn('trade_day', F.to_date('trade_date_brt'))
```

Pandas (para inspe√ß√£o local):

```python
import pandas as pd
df['trade_date_brt'] = pd.to_datetime(df['trade_date'], utc=True).dt.tz_convert('America/Sao_Paulo')
```

### Limita√ß√µes e boas pr√°ticas ‚ö†Ô∏è

- Intervalos intradi√°rios podem ser truncados pelo Yahoo; valide `period` √ó `interval` antes de rodar cargas longas. O script j√° emite avisos para combina√ß√µes potencialmente incompat√≠veis.
- N√£o converta timestamps no ingest ‚Äî armazene em **UTC** e converta na camada de consumo (Glue/Spark).
- Se precisar de preenchimento hor√°rio (holes), fa√ßa reindex downstream ou solicite que eu adicione `--fill-hours` como op√ß√£o no ingestor.

### Testes relacionados ‚úÖ

- H√° um teste unit√°rio que valida a normaliza√ß√£o de timestamps timezone-aware para UTC: `tests/test_timestamps_utc.py`.

## üõ†Ô∏è Detalhes T√©cnicos

- **Multi-Ticker:** O script processa uma lista de ativos sequencialmente.
- **Particionamento Real:** Diferente de scripts que usam a data de execu√ß√£o, este script identifica a data de cada registro no `yfinance` e cria a parti√ß√£o `date=YYYY-MM-DD` correspondente, permitindo cargas hist√≥ricas (`backfill`) precisas.
- **Tratamento de Colunas:** O script remove MultiIndex de colunas gerados pelo `yfinance` para garantir compatibilidade total com o AWS Glue e Athena.

## ‚úÖ Checklist de aceita√ß√£o

- [x] Ingest√£o raw em `raw/dt=YYYY-MM-DD/` (implementado)
- [ ] Glue job que gera `refined/date=.../ticker=.../` com: A (agrega√ß√£o), B (renomear 2 colunas), C (c√°lculo por data)
- [ ] Lambda acionada por evento S3 que inicia o Glue job (stub/teste)
- [ ] Glue Catalog atualizado e tabelas acess√≠veis via Athena
- [ ] Queries de valida√ß√£o (partition pruning e m√©tricas calculadas) inclu√≠das na documenta√ß√£o

---
*Este projeto faz parte da Fase 2 do Tech Challenge de Machine Learning Avan√ßado - FIAP.*
